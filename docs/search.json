[
  {
    "objectID": "ess330_Lab_6.html",
    "href": "ess330_Lab_6.html",
    "title": "ess330_Lab_6",
    "section": "",
    "text": "#Lab SetUp\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.0     \n\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\n\n\n#download data and get PDF\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\nGetting Basin characteristics\n\n#a\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n#b\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\n#c\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n#d\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\n#e\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n#Question 1:\nAnswer: The data files and PDF are downloaded and was placed into my data folder. In the PDF, zero_q_freq represents the frequency of days with Q = 0 mm/day and is represented as percentage.\n\n#make map of the sites\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()"
  },
  {
    "objectID": "ess330_Lab_6.html#lab-6",
    "href": "ess330_Lab_6.html#lab-6",
    "title": "ess330_Lab_6",
    "section": "",
    "text": "#Lab SetUp\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.0     \n\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\n\n\n#download data and get PDF\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\nGetting Basin characteristics\n\n#a\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n#b\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\n#c\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n#d\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\n#e\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n#Question 1:\nAnswer: The data files and PDF are downloaded and was placed into my data folder. In the PDF, zero_q_freq represents the frequency of days with Q = 0 mm/day and is represented as percentage.\n\n#make map of the sites\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()"
  },
  {
    "objectID": "ess330_Lab_6.html#question-2",
    "href": "ess330_Lab_6.html#question-2",
    "title": "ess330_Lab_6",
    "section": "Question 2",
    "text": "Question 2\n\n#make 2 maps\nlibrary(ggplot2)\nlibrary(ggthemes)\n\nWarning: package 'ggthemes' was built under R version 4.4.3\n\nlibrary(patchwork)  \n\n#Map 1: Aridity\nmap_aridity &lt;- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"tan\", high = \"darkred\") +\n  labs(title = \"CAMELS Basins Colored by Aridity\",\n       x = \"Longitude\", y = \"Latitude\", color = \"Aridity\") +\n  ggthemes::theme_map()\n\n#Map 2: Mean Precipitation\nmap_precip &lt;- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = p_mean)) +\n  scale_color_gradient(low = \"lightblue\", high = \"darkblue\") +\n  labs(title = \"CAMELS Basins Colored by Mean Precipitation (p_mean)\",\n       x = \"Longitude\", y = \"Latitude\", color = \"p_mean (mm)\") +\n  ggthemes::theme_map()\n\n#Combine \nmap_aridity + map_precip\n\n\n\n\n\n\n\n\n\n#Model Prep\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n#visual EDA\n\n#Looking at 3 variables\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n#Model Building\n\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n#preprocessor\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n#Naive base lm approach\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n#wrong version 1\n\nnrow(camels_test)\n\n[1] 135\n\nnrow(camels_train)\n\n[1] 536\n\n\n#Wrong version 2\n\ncamels_test$p2 = predict(lm_base, newdata = camels_test)\n\n## Scales way off!\nggplot(camels_test, aes(x = p2, y = logQmean)) + \n  geom_point() + \n  # Linear fit line, no error bands\n  geom_smooth(method = \"lm\", se = FALSE, size =1) +\n  # 1:1 line\n  geom_abline(color = \"red\", size = 1) + \n  labs(title = \"Linear Model Using `predict()`\",\n       x = \"Predicted Log Mean Flow\",\n       y = \"Observed Log Mean Flow\") + \n  theme_linedraw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n#Correct version\n\n#preprocessing\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n#model Eval.\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n#view\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n#Using wrokflow\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n#Making Predictions\n\n#\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  62\n\n\n#Model Eval: Stats. and visual\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n#Switch it up\n\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\n#predictions\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  61\n\n#model eval.\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.588\n2 rsq     standard       0.740\n3 mae     standard       0.365\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n#a workflow approach\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.564  0.0253    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.770  0.0260    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2"
  },
  {
    "objectID": "ess330_Lab_6.html#question-3-build-2-models",
    "href": "ess330_Lab_6.html#question-3-build-2-models",
    "title": "ess330_Lab_6",
    "section": "Question 3: Build 2 models",
    "text": "Question 3: Build 2 models\n\n#Model 1\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nxgb_model &lt;- boost_tree(trees = 1000) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\n#Model 2\nnn_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n\n#combine models in workflow set\nmodels &lt;- list(\n  lm_model,\n  rf_model,\n  xgb_model,\n  nn_model\n)\n\nwf_set &lt;- workflow_set(\n  preproc = list(rec),\n  models = models\n) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = camels_cv)\n#evaluate\nautoplot(wf_set) \n\n\n\n\n\n\n\n#rank by R-squared\nrank_results(wf_set, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.541  0.0257    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.793  0.0240    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.563  0.0255    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.771  0.0258    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.640  0.0290    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.716  0.0289    10 recipe       boos…     4\n\n\n#answer: After evaluating the 4 models, I would go with the nueral network was the best performing. With the highest R-squared and the lowest RMSE across the 10 fold cross validation."
  },
  {
    "objectID": "ess330_Lab_6.html#question-4-build-your-own",
    "href": "ess330_Lab_6.html#question-4-build-your-own",
    "title": "ess330_Lab_6",
    "section": "Question 4: Build your own",
    "text": "Question 4: Build your own\n#Q4a: data splitting\n\nset.seed(123) \n\ncamels_split_4 &lt;- initial_split(camels, prop = 0.75)\ncamels_train_4 &lt;- training(camels_split_4)\ncamels_test_4  &lt;- testing(camels_split_4)\n\ncamels_cv_4 &lt;- vfold_cv(camels_train_4, v = 10)\n\n#Q4b: Recipe\n\nrec_4 &lt;- recipe(logQmean ~ area_gages2 + slope_mean + p_mean + pet_mean + aridity + frac_forest, \n                data = camels_train_4) %&gt;%\n  step_log(all_predictors(), offset = 1e-6) %&gt;%  \n  step_normalize(all_predictors()) %&gt;%\n  step_naomit(all_predictors(), all_outcomes())\n\n#answer I chose this formula because it includes key physical and climatic drivers of streamflow: basin area, slope, precipitation, evapotranspiration, aridity, and forest cover. These variables influence water availability and movement across the landscape.\n#Q4c Define 3 models\n\n#model 1\nrf_4 &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n#model 2\nxgb_4 &lt;- boost_tree(trees = 1000) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n#model 3\nnn_4 &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n#Q4d workflow set\n\nwf_4 &lt;- workflow_set(\n  preproc = list(rec_4),  \n  models = list(\n    random_forest = rf_4,\n    xgboost = xgb_4,\n    neural_net = nn_4\n  )\n) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = camels_cv_4)\n\n#Q4e evaluation\n\n#autoplot\nautoplot(wf_4)\n\n\n\n\n\n\n\n#Rank\nrank_results(wf_4, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_xgboost    Prepro… rmse    0.337 0.0221     10 recipe       boos…     1\n2 recipe_xgboost    Prepro… rsq     0.920 0.00870    10 recipe       boos…     1\n3 recipe_random_fo… Prepro… rmse    0.364 0.0220     10 recipe       rand…     2\n4 recipe_random_fo… Prepro… rsq     0.910 0.00836    10 recipe       rand…     2\n5 recipe_neural_net Prepro… rmse    0.357 0.0270     10 recipe       bag_…     3\n6 recipe_neural_net Prepro… rsq     0.909 0.0129     10 recipe       bag_…     3\n\n\n#answer: Out of the three models tested, the XGBoost model was the best performing. It had the lowest RMSE and the highest R^2 across all the resamples. This means it was the most reliable predictor of mean stream flow.\n#Q4f Extact and Evaluate\n\n#Build final workflow\nfinal_wf &lt;- workflow() %&gt;%\n  add_recipe(rec_4) %&gt;%\n  add_model(xgb_4)\n\n#Fit the model\nfinal_fit &lt;- final_wf %&gt;%\n  fit(data = camels_train_4)\n\n#Predict\nfinal_preds &lt;- augment(final_fit, new_data = camels_test_4)\n\n#Get metrics\nmetrics(final_preds, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.374\n2 rsq     standard       0.900\n3 mae     standard       0.237\n\n#Plot observed vs predicted\nggplot(final_preds, aes(x = logQmean, y = .pred, color = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline(linetype = \"dashed\") +\n  labs(\n    title = \"XGBoost Final Model: Observed vs Predicted Log Mean Flow\",\n    x = \"Observed Log Mean Flow\",\n    y = \"Predicted Log Mean Flow\",\n    color = \"Aridity\"\n  ) +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n#answer: The XGBoost model performed well overall, especially for lower aridity values where most predictions lined up closely with observed values. As aridity increased, predictions became more spread out, which makes sense since flow is harder to predict in more arid regions. Still, the model held up well and shows strong potential for general streamflow prediction."
  }
]