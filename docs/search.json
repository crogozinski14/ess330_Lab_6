[
  {
    "objectID": "ess330_Lab_6.html",
    "href": "ess330_Lab_6.html",
    "title": "ess330_Lab_6",
    "section": "",
    "text": "#Lab SetUp\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.0     \n\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\n\n\n#download data and get PDF\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\nGetting Basin characteristics\n\n#a\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n#b\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\n#c\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n#d\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\n#e\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n#Question 1:\nAnswer: The data files and PDF are downloaded and was placed into my data folder. In the PDF, zero_q_freq represents the frequency of days with Q = 0 mm/day and is represented as percentage.\n\n#make map of the sites\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()"
  },
  {
    "objectID": "ess330_Lab_6.html#lab-6",
    "href": "ess330_Lab_6.html#lab-6",
    "title": "ess330_Lab_6",
    "section": "",
    "text": "#Lab SetUp\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.0     \n\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\n\n\n#download data and get PDF\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\nGetting Basin characteristics\n\n#a\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n#b\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\n#c\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n#d\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\n#e\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n#Question 1:\nAnswer: The data files and PDF are downloaded and was placed into my data folder. In the PDF, zero_q_freq represents the frequency of days with Q = 0 mm/day and is represented as percentage.\n\n#make map of the sites\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()"
  },
  {
    "objectID": "ess330_Lab_6.html#question-2",
    "href": "ess330_Lab_6.html#question-2",
    "title": "ess330_Lab_6",
    "section": "Question 2",
    "text": "Question 2\n\n#make 2 maps\nlibrary(ggplot2)\nlibrary(ggthemes)\n\nWarning: package 'ggthemes' was built under R version 4.4.3\n\nlibrary(patchwork)  \n\n#Map 1: Aridity\nmap_aridity &lt;- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"tan\", high = \"darkred\") +\n  labs(title = \"CAMELS Basins Colored by Aridity\",\n       x = \"Longitude\", y = \"Latitude\", color = \"Aridity\") +\n  ggthemes::theme_map()\n\n#Map 2: Mean Precipitation\nmap_precip &lt;- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = p_mean)) +\n  scale_color_gradient(low = \"lightblue\", high = \"darkblue\") +\n  labs(title = \"CAMELS Basins Colored by Mean Precipitation (p_mean)\",\n       x = \"Longitude\", y = \"Latitude\", color = \"p_mean (mm)\") +\n  ggthemes::theme_map()\n\n#Combine \nmap_aridity + map_precip\n\n\n\n\n\n\n\n\n\n#Model Prep\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n#visual EDA\n\n#Looking at 3 variables\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n#Model Building\n\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n#preprocessor\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n#Naive base lm approach\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n#wrong version 1\n\nnrow(camels_test)\n\n[1] 135\n\nnrow(camels_train)\n\n[1] 536\n\n\n#Wrong version 2\n\ncamels_test$p2 = predict(lm_base, newdata = camels_test)\n\n## Scales way off!\nggplot(camels_test, aes(x = p2, y = logQmean)) + \n  geom_point() + \n  # Linear fit line, no error bands\n  geom_smooth(method = \"lm\", se = FALSE, size =1) +\n  # 1:1 line\n  geom_abline(color = \"red\", size = 1) + \n  labs(title = \"Linear Model Using `predict()`\",\n       x = \"Predicted Log Mean Flow\",\n       y = \"Observed Log Mean Flow\") + \n  theme_linedraw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n#Correct version\n\n#preprocessing\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n#model Eval.\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n#view\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n#Using wrokflow\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n#Making Predictions\n\n#\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  62\n\n\n#Model Eval: Stats. and visual\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n#Switch it up\n\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\n#predictions\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  61\n\n#model eval.\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.588\n2 rsq     standard       0.740\n3 mae     standard       0.365\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n#a workflow approach\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.564  0.0253    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.770  0.0260    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2"
  },
  {
    "objectID": "ess330_Lab_6.html#question-3-build-2-models",
    "href": "ess330_Lab_6.html#question-3-build-2-models",
    "title": "ess330_Lab_6",
    "section": "Question 3: Build 2 models",
    "text": "Question 3: Build 2 models\n\n#Model 1\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nxgb_model &lt;- boost_tree(trees = 1000) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\n#Model 2\nnn_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n\n#combine models in workflow set\nmodels &lt;- list(\n  lm_model,\n  rf_model,\n  xgb_model,\n  nn_model\n)\n\nwf_set &lt;- workflow_set(\n  preproc = list(rec),\n  models = models\n) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = camels_cv)\n#evaluate\nautoplot(wf_set) \n\n\n\n\n\n\n\n#rank by R-squared\nrank_results(wf_set, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.541  0.0257    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.793  0.0240    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.563  0.0255    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.771  0.0258    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.640  0.0290    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.716  0.0289    10 recipe       boos…     4\n\n\n#answer: After evaluating the 4 models, I would go with the nueral network was the best performing. With the highest R-squared and the lowest RMSE across the 10 fold cross validation."
  },
  {
    "objectID": "ess330_Lab_6.html#question-4-build-your-own",
    "href": "ess330_Lab_6.html#question-4-build-your-own",
    "title": "ess330_Lab_6",
    "section": "Question 4: Build your own",
    "text": "Question 4: Build your own\n#Q4a: data splitting\n\nset.seed(123) \n\ncamels_split_4 &lt;- initial_split(camels, prop = 0.75)\ncamels_train_4 &lt;- training(camels_split_4)\ncamels_test_4  &lt;- testing(camels_split_4)\n\ncamels_cv_4 &lt;- vfold_cv(camels_train_4, v = 10)\n\n#Q4b: Recipe\n\nrec_4 &lt;- recipe(logQmean ~ area_gages2 + slope_mean + p_mean + pet_mean + aridity + frac_forest, \n                data = camels_train_4) %&gt;%\n  step_log(all_predictors(), offset = 1e-6) %&gt;%  \n  step_normalize(all_predictors()) %&gt;%\n  step_naomit(all_predictors(), all_outcomes())\n\n#answer I chose this formula because it includes key physical and climatic drivers of streamflow: basin area, slope, precipitation, evapotranspiration, aridity, and forest cover. These variables influence water availability and movement across the landscape.\n#Q4c Define 3 models\n\n#model 1\nrf_4 &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n#model 2\nxgb_4 &lt;- boost_tree(trees = 1000) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n#model 3\nnn_4 &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n#Q4d workflow set\n\nwf_4 &lt;- workflow_set(\n  preproc = list(rec_4),  \n  models = list(\n    random_forest = rf_4,\n    xgboost = xgb_4,\n    neural_net = nn_4\n  )\n) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = camels_cv_4)\n\n#Q4e evaluation\n\n#autoplot\nautoplot(wf_4)\n\n\n\n\n\n\n\n#Rank\nrank_results(wf_4, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_xgboost    Prepro… rmse    0.337 0.0221     10 recipe       boos…     1\n2 recipe_xgboost    Prepro… rsq     0.920 0.00870    10 recipe       boos…     1\n3 recipe_random_fo… Prepro… rmse    0.364 0.0220     10 recipe       rand…     2\n4 recipe_random_fo… Prepro… rsq     0.910 0.00836    10 recipe       rand…     2\n5 recipe_neural_net Prepro… rmse    0.357 0.0270     10 recipe       bag_…     3\n6 recipe_neural_net Prepro… rsq     0.909 0.0129     10 recipe       bag_…     3\n\n\n#answer: Out of the three models tested, the XGBoost model was the best performing. It had the lowest RMSE and the highest R^2 across all the resamples. This means it was the most reliable predictor of mean stream flow.\n#Q4f Extact and Evaluate\n\n#Build final workflow\nfinal_wf &lt;- workflow() %&gt;%\n  add_recipe(rec_4) %&gt;%\n  add_model(xgb_4)\n\n#Fit the model\nfinal_fit &lt;- final_wf %&gt;%\n  fit(data = camels_train_4)\n\n#Predict\nfinal_preds &lt;- augment(final_fit, new_data = camels_test_4)\n\n#Get metrics\nmetrics(final_preds, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.374\n2 rsq     standard       0.900\n3 mae     standard       0.237\n\n#Plot observed vs predicted\nggplot(final_preds, aes(x = logQmean, y = .pred, color = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline(linetype = \"dashed\") +\n  labs(\n    title = \"XGBoost Final Model: Observed vs Predicted Log Mean Flow\",\n    x = \"Observed Log Mean Flow\",\n    y = \"Predicted Log Mean Flow\",\n    color = \"Aridity\"\n  ) +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n#answer: The XGBoost model performed well overall, especially for lower aridity values where most predictions lined up closely with observed values. As aridity increased, predictions became more spread out, which makes sense since flow is harder to predict in more arid regions. Still, the model held up well and shows strong potential for general streamflow prediction."
  },
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "hyperparameter-tuning",
    "section": "",
    "text": "##Lab 8"
  },
  {
    "objectID": "hyperparameter-tuning.html#data-importtidytransform",
    "href": "hyperparameter-tuning.html#data-importtidytransform",
    "title": "hyperparameter-tuning",
    "section": "Data Import/Tidy/Transform",
    "text": "Data Import/Tidy/Transform\n\n#load required packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.0     \n\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(readr)\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.4.3\n\nlibrary(visdat)\n\nWarning: package 'visdat' was built under R version 4.4.3\n\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.4.3"
  },
  {
    "objectID": "hyperparameter-tuning.html#read-in-data",
    "href": "hyperparameter-tuning.html#read-in-data",
    "title": "hyperparameter-tuning",
    "section": "Read in data",
    "text": "Read in data\n\nlibrary(tidyverse)\nlibrary(powerjoin)\n\n# List all attribute files\nattribute_files &lt;- list.files(\"data\", pattern = \"camels_.*\\\\.txt$\", full.names = TRUE)\n\n# Define a function to read each file with the correct delimiter\nread_camels_file &lt;- function(file) {\n  read_delim(\n    file,\n    delim = \";\",\n    col_types = cols(.default = col_double(), gauge_id = col_character()),\n    trim_ws = TRUE\n  )\n}\n\n# Read each file and join by `gauge_id`\ncamels_list &lt;- attribute_files %&gt;%\n  set_names(basename(.)) %&gt;%\n  map(read_camels_file)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\nOne or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\nOne or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n# Merge all datasets\ncamels &lt;- reduce(camels_list, power_full_join, by = \"gauge_id\")\n\n# Preview structure\nglimpse(camels)\n\nRows: 671\nColumns: 58\n$ gauge_id             &lt;chr&gt; \"01013500\", \"01022500\", \"01030500\", \"01031500\", \"…\n$ p_mean               &lt;dbl&gt; 3.126679, 3.608126, 3.274405, 3.522957, 3.323146,…\n$ pet_mean             &lt;dbl&gt; 1.971555, 2.119256, 2.043594, 2.071324, 2.090024,…\n$ p_seasonality        &lt;dbl&gt; 0.187940259, -0.114529586, 0.047358189, 0.1040905…\n$ frac_snow            &lt;dbl&gt; 0.3134404, 0.2452590, 0.2770184, 0.2918365, 0.280…\n$ aridity              &lt;dbl&gt; 0.6305587, 0.5873564, 0.6241114, 0.5879503, 0.628…\n$ high_prec_freq       &lt;dbl&gt; 12.95, 20.55, 17.15, 18.90, 20.10, 13.50, 17.50, …\n$ high_prec_dur        &lt;dbl&gt; 1.348958, 1.205279, 1.207746, 1.148936, 1.165217,…\n$ high_prec_timing     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ low_prec_freq        &lt;dbl&gt; 202.20, 233.65, 215.60, 227.35, 235.90, 193.50, 2…\n$ low_prec_dur         &lt;dbl&gt; 3.427119, 3.662226, 3.514262, 3.473644, 3.691706,…\n$ low_prec_timing      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ geol_1st_class       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ glim_1st_class_frac  &lt;dbl&gt; 0.8159044, 0.5906582, 0.5733054, 0.4489279, 0.308…\n$ geol_2nd_class       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ glim_2nd_class_frac  &lt;dbl&gt; 0.17972945, 0.16461821, 0.28701001, 0.44386282, 0…\n$ carbonate_rocks_frac &lt;dbl&gt; 0.000000000, 0.000000000, 0.052140094, 0.02625797…\n$ geol_porostiy        &lt;dbl&gt; 0.1714, 0.0710, 0.1178, 0.0747, 0.0522, 0.0711, 0…\n$ geol_permeability    &lt;dbl&gt; -14.7019, -14.2138, -14.4918, -14.8410, -14.4819,…\n$ q_mean               &lt;dbl&gt; 1.699155, 2.173062, 1.820108, 2.030242, 2.182870,…\n$ runoff_ratio         &lt;dbl&gt; 0.5434375, 0.6022689, 0.5558590, 0.5762893, 0.656…\n$ slope_fdc            &lt;dbl&gt; 1.528219, 1.776280, 1.871110, 1.494019, 1.415939,…\n$ baseflow_index       &lt;dbl&gt; 0.5852260, 0.5544784, 0.5084407, 0.4450905, 0.473…\n$ stream_elas          &lt;dbl&gt; 1.8453242, 1.7027824, 1.3775052, 1.6486930, 1.510…\n$ q5                   &lt;dbl&gt; 0.24110613, 0.20473436, 0.10714920, 0.11134535, 0…\n$ q95                  &lt;dbl&gt; 6.373021, 7.123049, 6.854887, 8.010503, 8.095148,…\n$ high_q_freq          &lt;dbl&gt; 6.10, 3.90, 12.25, 18.90, 14.95, 14.10, 16.05, 16…\n$ high_q_dur           &lt;dbl&gt; 8.714286, 2.294118, 7.205882, 3.286957, 2.577586,…\n$ low_q_freq           &lt;dbl&gt; 41.35, 65.15, 89.25, 94.80, 71.55, 58.90, 82.20, …\n$ low_q_dur            &lt;dbl&gt; 20.170732, 17.144737, 19.402174, 14.697674, 12.77…\n$ zero_q_freq          &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0…\n$ hfd_mean             &lt;dbl&gt; 207.25, 166.25, 184.90, 181.00, 184.80, 197.20, 1…\n$ soil_depth_pelletier &lt;dbl&gt; 7.4047619, 17.4128079, 19.0114144, 7.2525570, 5.3…\n$ soil_depth_statsgo   &lt;dbl&gt; 1.248408, 1.491846, 1.461363, 1.279047, 1.392779,…\n$ soil_porosity        &lt;dbl&gt; 0.4611488, 0.4159055, 0.4590910, 0.4502360, 0.422…\n$ soil_conductivity    &lt;dbl&gt; 1.106522, 2.375005, 1.289807, 1.373292, 2.615154,…\n$ max_water_content    &lt;dbl&gt; 0.5580548, 0.6262289, 0.6530198, 0.5591227, 0.561…\n$ sand_frac            &lt;dbl&gt; 27.84183, 59.39016, 32.23546, 35.26903, 55.16313,…\n$ silt_frac            &lt;dbl&gt; 55.15694, 28.08094, 51.77918, 50.84123, 34.18544,…\n$ clay_frac            &lt;dbl&gt; 16.275732, 12.037646, 14.776824, 12.654125, 10.30…\n$ water_frac           &lt;dbl&gt; 5.3766978, 1.2269127, 1.6343449, 0.6745936, 0.000…\n$ organic_frac         &lt;dbl&gt; 0.4087168, 0.0000000, 1.3302776, 0.0000000, 0.000…\n$ other_frac           &lt;dbl&gt; 0.0000000, 0.3584723, 0.0220161, 0.0000000, 0.147…\n$ gauge_lat            &lt;dbl&gt; 47.23739, 44.60797, 45.50097, 45.17501, 44.86920,…\n$ gauge_lon            &lt;dbl&gt; -68.58264, -67.93524, -68.30596, -69.31470, -69.9…\n$ elev_mean            &lt;dbl&gt; 250.31, 92.68, 143.80, 247.80, 310.38, 615.70, 47…\n$ slope_mean           &lt;dbl&gt; 21.64152, 17.79072, 12.79195, 29.56035, 49.92122,…\n$ area_gages2          &lt;dbl&gt; 2252.70, 573.60, 3676.17, 769.05, 909.10, 383.82,…\n$ area_geospa_fabric   &lt;dbl&gt; 2303.95, 620.38, 3676.09, 766.53, 904.94, 396.10,…\n$ frac_forest          &lt;dbl&gt; 0.9063, 0.9232, 0.8782, 0.9548, 0.9906, 1.0000, 1…\n$ lai_max              &lt;dbl&gt; 4.167304, 4.871392, 4.685200, 4.903259, 5.086811,…\n$ lai_diff             &lt;dbl&gt; 3.340732, 3.746692, 3.665543, 3.990843, 4.300978,…\n$ gvf_max              &lt;dbl&gt; 0.8045674, 0.8639358, 0.8585020, 0.8706685, 0.891…\n$ gvf_diff             &lt;dbl&gt; 0.3716482, 0.3377125, 0.3513934, 0.3986194, 0.445…\n$ dom_land_cover_frac  &lt;dbl&gt; 0.8834519, 0.8204934, 0.9752580, 1.0000000, 0.850…\n$ dom_land_cover       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ root_depth_50        &lt;dbl&gt; NA, 0.2374345, NA, 0.2500000, 0.2410270, 0.225615…\n$ root_depth_99        &lt;dbl&gt; NA, 2.238444, NA, 2.400000, 2.340180, 2.237435, 2…\n\n\n\n#remove columns with missing values\ncamels_clean &lt;- camels %&gt;%\n  select(where(~ mean(!is.na(.)) &gt; 0.5))  \n\n#drop rows missing values\ncamels_clean &lt;- camels_clean %&gt;%\n  drop_na(runoff_ratio)\n\ncamels_for_map &lt;- camels_clean  \n\ncamels_model &lt;- camels_for_map %&gt;%\n  select(-gauge_id, -gauge_lat, -gauge_lon)\n\n\n\n#check columns\ncamels_clean %&gt;%\n  select(where(is.character)) %&gt;%\n  names()\n\n[1] \"gauge_id\""
  },
  {
    "objectID": "hyperparameter-tuning.html#data-splitting",
    "href": "hyperparameter-tuning.html#data-splitting",
    "title": "hyperparameter-tuning",
    "section": "Data Splitting",
    "text": "Data Splitting\n\n#set seed, split data, and extract sets\nset.seed(123)\n\ndata_split &lt;- initial_split(camels_model, prop = 0.8)\n\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)"
  },
  {
    "objectID": "hyperparameter-tuning.html#feature-engineering",
    "href": "hyperparameter-tuning.html#feature-engineering",
    "title": "hyperparameter-tuning",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n#create recipe\nqmean_recipe &lt;- recipe(q_mean ~ ., data = train_data) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_corr(all_predictors(), threshold = 0.9) %&gt;%\n  step_nzv(all_predictors())"
  },
  {
    "objectID": "hyperparameter-tuning.html#model-tuning",
    "href": "hyperparameter-tuning.html#model-tuning",
    "title": "hyperparameter-tuning",
    "section": "Model Tuning",
    "text": "Model Tuning\n#1 Build a model\n\nrf_tune_model &lt;- rand_forest(\n  mtry = tune(),      \n  min_n = tune(),      \n  trees = 500           \n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n#2 Create a Workflow\n\nrf_tune_workflow &lt;- workflow() %&gt;%\n  add_model(rf_tune_model) %&gt;%\n  add_recipe(qmean_recipe)\n\n#3 Check the Tunable Values\n\n#extract parameters\ndials &lt;- extract_parameter_set_dials(rf_tune_workflow)\n\n#view the tuning parameters\ndials$object\n\n[[1]]\n# Randomly Selected Predictors (quantitative)\nRange: [1, ?]\n\n[[2]]\nMinimal Node Size (quantitative)\nRange: [2, 40]\n\n\n#4. Define the Search Space\n\n#finalize the parameter\ndials_final &lt;- finalize(dials, train_data)\n\n#create grid\nmy.grid &lt;- grid_space_filling(\n  dials_final,\n  size = 25\n)\n\n#5 Tune the Model\n\nmodel_params &lt;- tune_grid(\n  rf_tune_workflow,\n  resamples = cv_folds,\n  grid = my.grid,\n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n\n→ A | warning: 45 columns were requested but there were 44 predictors in the data. 44 will be used.\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: 47 columns were requested but there were 44 predictors in the data. 44 will be used.\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1   B: x1\n→ C | warning: 50 columns were requested but there were 44 predictors in the data. 44 will be used.\nThere were issues with some computations   A: x1   B: x1\nThere were issues with some computations   A: x1   B: x1   C: x1\nThere were issues with some computations   A: x2   B: x1   C: x1\nThere were issues with some computations   A: x2   B: x2   C: x1\nThere were issues with some computations   A: x2   B: x2   C: x2\nThere were issues with some computations   A: x3   B: x2   C: x2\nThere were issues with some computations   A: x3   B: x3   C: x2\nThere were issues with some computations   A: x3   B: x3   C: x3\nThere were issues with some computations   A: x4   B: x3   C: x3\nThere were issues with some computations   A: x4   B: x4   C: x3\nThere were issues with some computations   A: x4   B: x4   C: x4\nThere were issues with some computations   A: x5   B: x4   C: x4\nThere were issues with some computations   A: x5   B: x5   C: x4\nThere were issues with some computations   A: x5   B: x5   C: x5\nThere were issues with some computations   A: x6   B: x5   C: x5\nThere were issues with some computations   A: x6   B: x6   C: x5\nThere were issues with some computations   A: x6   B: x6   C: x6\nThere were issues with some computations   A: x7   B: x6   C: x6\nThere were issues with some computations   A: x7   B: x7   C: x6\nThere were issues with some computations   A: x7   B: x7   C: x7\nThere were issues with some computations   A: x8   B: x7   C: x7\nThere were issues with some computations   A: x8   B: x8   C: x7\nThere were issues with some computations   A: x8   B: x8   C: x8\nThere were issues with some computations   A: x9   B: x8   C: x8\nThere were issues with some computations   A: x9   B: x9   C: x8\nThere were issues with some computations   A: x9   B: x9   C: x9\nThere were issues with some computations   A: x10   B: x9   C: x9\nThere were issues with some computations   A: x10   B: x10   C: x9\nThere were issues with some computations   A: x10   B: x10   C: x10\nThere were issues with some computations   A: x10   B: x10   C: x10\n\nautoplot(model_params)\n\n\n\n\n\n\n\n\n#Answer:It looks like models with lower mtry values performed best — they had the lowest error and highest R². Performance dropped as mtry increased beyond ~10–15. min_n didn’t show a clear pattern, which suggests it wasn’t as influential for this dataset.\n#6. Check the skill of the tuned model\n\n#collect metrics\ncollect_metrics(model_params)\n\n# A tibble: 75 × 8\n    mtry min_n .metric .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     1    24 mae     standard   0.391    10 0.0171  Preprocessor1_Model01\n 2     1    24 rmse    standard   0.654    10 0.0488  Preprocessor1_Model01\n 3     1    24 rsq     standard   0.914    10 0.00969 Preprocessor1_Model01\n 4     3    13 mae     standard   0.216    10 0.0149  Preprocessor1_Model02\n 5     3    13 rmse    standard   0.392    10 0.0409  Preprocessor1_Model02\n 6     3    13 rsq     standard   0.959    10 0.00644 Preprocessor1_Model02\n 7     5    33 mae     standard   0.201    10 0.0151  Preprocessor1_Model03\n 8     5    33 rmse    standard   0.398    10 0.0420  Preprocessor1_Model03\n 9     5    33 rsq     standard   0.955    10 0.00714 Preprocessor1_Model03\n10     7     5 mae     standard   0.137    10 0.0113  Preprocessor1_Model04\n# ℹ 65 more rows\n\nshow_best(model_params, metric = \"mae\")\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1    17     3 mae     standard   0.122    10 0.00868 Preprocessor1_Model09\n2    15    11 mae     standard   0.125    10 0.00922 Preprocessor1_Model08\n3    19    21 mae     standard   0.135    10 0.00888 Preprocessor1_Model10\n4     7     5 mae     standard   0.137    10 0.0113  Preprocessor1_Model04\n5     9    19 mae     standard   0.140    10 0.00994 Preprocessor1_Model05\n\n\n#answer: The best-performing model based on MAE had mtry = 1 and min_n = 24. This combo gave the lowest average absolute error, meaning it predicted q_mean values most consistently across folds.\n\nhp_best &lt;- select_best(model_params, metric = \"mae\")\n\n#7. Finalizing the model\n\n#finalize and fit the model\nfinal_rf_workflow &lt;- finalize_workflow(rf_tune_workflow, hp_best)\n\nfinal_rf_fit &lt;- fit(final_rf_workflow, data = train_data)"
  },
  {
    "objectID": "hyperparameter-tuning.html#final-model-verification",
    "href": "hyperparameter-tuning.html#final-model-verification",
    "title": "hyperparameter-tuning",
    "section": "Final Model Verification",
    "text": "Final Model Verification\n\n#final fit\nfinal_fit &lt;- last_fit(\n  final_rf_workflow,\n  split = data_split\n)\n\n#evaluate\ncollect_metrics(final_fit)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.198 Preprocessor1_Model1\n2 rsq     standard       0.982 Preprocessor1_Model1\n\n\n#answer: The final model performed really well on the test set, with low RMSE and high R². This means the predictions were close to the actual values and the model explained nearly all the variance in q_mean. The performance is consistent with training, so the model generalized well.\n\nfinal_preds &lt;- collect_predictions(final_fit)\n\nfinal_fit &lt;- last_fit(\n  final_rf_workflow,\n  split = data_split\n)\n\n\n#ggplot\nggplot(final_preds, aes(x = .pred, y = q_mean)) +\n  geom_point(aes(color = .pred), alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkblue\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  scale_color_viridis_c(option = \"C\") +\n  labs(\n    title = \"Predicted vs Actual q_mean (Test Set)\",\n    x = \"Predicted q_mean\",\n    y = \"Actual q_mean\",\n    color = \"Prediction\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "hyperparameter-tuning.html#building-a-map",
    "href": "hyperparameter-tuning.html#building-a-map",
    "title": "hyperparameter-tuning",
    "section": "Building a Map",
    "text": "Building a Map\n\n#fit final model to cleaned data\nfull_fit &lt;- fit(final_rf_workflow, data = camels_clean)\n\n#predict\nlibrary(broom)\n\ncamels_aug &lt;- augment(full_fit, new_data = camels_clean)\n\n#add residuals\ncamels_aug &lt;- camels_aug %&gt;%\n  mutate(\n    residual = (.pred - q_mean)^2\n  )\n\n#create maps\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\n#prediction map\npred_map &lt;- ggplot(camels_aug, aes(x = gauge_lon, y = gauge_lat, color = .pred)) +\n  geom_point(size = 2) +\n  scale_color_viridis_c(option = \"C\") +\n  labs(title = \"Predicted q_mean\", color = \"Prediction\") +\n  coord_fixed(1.3) +\n  theme_minimal()\n\n#residual map\nresid_map &lt;- ggplot(camels_aug, aes(x = gauge_lon, y = gauge_lat, color = residual)) +\n  geom_point(size = 2) +\n  scale_color_viridis_c(option = \"C\") +\n  labs(title = \"Prediction Residuals (Squared Error)\", color = \"Residual\") +\n  coord_fixed(1.3) +\n  theme_minimal()\n\n#combine\npred_map + resid_map"
  }
]